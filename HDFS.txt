- Namenode maintains metadata in memory and always whenever the request comes, it gets the data from memory, because namenode needs to be very fast. For persistant storage namenode stores the state in fsimage all the intermediate changes, it stores in edits log file. Namenode merges fsimage with editslog at the startup. Secondary namenode or edits log. The namenode also knows the datanodes on which all the blocks for a given file are located; however, it does not store block locations persistently, because this information is reconstructed from datanodes when the system starts.

HDFS over HTTP
	- WebHDFS, the embedded web servers in the namenode and datanodes act as WebHDFS endpoints. (WebHDFS is enabled by default, since dfs.webhdfs.enabled is set to true.) File metadata operations are handled by the namenode, while file read (and write) operations are sent first to the namenode, which sends an HTTP redirect to the client indicating the datanode to stream file data from (or to).
	- HttpFS,  All traffic to the cluster passes through the proxy, so the client never accesses the namenode or datanode directly. This allows for stricter firewall and bandwidth-limiting policies to be put in place. It’s common to use a proxy for transfers between Hadoop clusters located in different data centers, or when accessing a Hadoop cluster running in the cloud from an external network. The HttpFS proxy exposes the same HTTP (and HTTPS) interface as WebHDFS, so clients can access both using webhdfs (or swebhdfs) URIs. The HttpFS proxy is started independently of the namenode and datanode daemons, using the httpfs.sh script, and by default listens on a different port number (14000).

- Namenode configurations
	dfs.namenode.checkpoint.period - Set to one hour by default and specified the consequetive delay between two checkpoints
	dfs.namenode.checkpoint.txns, set to 1 million by default, defines the number of uncheckpointed transactions on the NameNode which will force an urgent checkpoint, even if the checkpoint period has not been reached.

- Secondary namenode does not have option of uploadToNameNode. This is the option provided by checkpoint node. Checkpoint node is same as secondary namenode but also provides uploadToNameNode. In the case of secondary namenode, namenode need to fetch the merged fsimage file from secondary namenode.

- The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage.

- The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files.

- All HDFS communications are on top of TCP/IP,  A client establishes a connection to a configurable TCP port on the NameNode machine.  It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.

- Failures
	- Datanode failure
		- If namenode does not sense the heartbeat od datanode then it means that datanode failed. In this case namenode re-replicate the blocks of the datanode.
	- Data corruption
		The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
	- Staging
		A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the NameNode dies before the file is closed, the file is lost.
	- Replication Pipelining
		When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions, writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.
- File delete and undelete
	- When file gets deleted, it moves to trash dir in HDFS. It stays in trash dir for cofigured trash interval time. Current default trash interval is set to 0 (Deletes file without storing in trash). This value is configurable parameter stored as fs.trash.interval stored in core-site.xml.
- HDFS commands
	- hdfs fsck 
		- Runs a HDFS filesystem checking utility.  
	- hdfs balancer 
		- Runs a cluster balancing utility. 
	-  hdfs mover
		- Runs the data migration utility. 
	- hdfs fs
		- This command supports operations on HDFS,Local FS, HFTP FS, S3 FS
			- hdfs dfs -appendToFile <localsrc> ... <dst>
				- Examples
					hdfs dfs -appendToFile localfile /user/hadoop/hadoopfile
					hdfs dfs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile
					hdfs dfs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile
					hdfs dfs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.
			-  hdfs dfs -cat URI [URI ...]
				- displays files
			- hdfs dfs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI ...]
				- changes the mode of the file
			- hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
			- hdfs dfs -copyFromLocal <localsrc> URI
			- hdfs dfs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
			- hdfs dfs -count [-q] [-h] <paths>
				Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns with -count are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE FILE_NAME
			- hdfs dfs -cp [-f] [-p | -p[topax]] URI [URI ...] <dest>
				- Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory.
				- The -p option will preserve file attributes [topx] (timestamps, ownership, permission, ACL, XAttr). If -p is specified with no arg, then preserves timestamps, ownership, permission. 
				- hdfs dfs -du [-s] [-h] URI [URI ...]
					- Displays sizes of files and directories contained in the given directory or the length of a file in case its just a file.
					- The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files.
					- The -h option will format file sizes in a "human-readable" fashion (e.g 64.0m instead of 67108864)
			- hdfs dfs -expunge
				- Empty the trash
			- hdfs dfs -ls [-R] <args>
			- hdfs dfs -mkdir 
			- hdfs dfs -moveFromLocal <localsrc> <dst>
			-  hdfs dfs -mv URI [URI ...] <dest>
				- Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across file systems is not permitted.
			- hdfs dfs -rm [-f] [-r|-R] [-skipTrash] URI [URI ...]
				- -r and -R are same
			-  hdfs dfs -setrep [-w] <numReplicas> <path>
				- Changes the replication factor of a file. If path is a directory then the command recursively changes the replication factor of all files under the directory tree rooted at path.
				- The -w flag requests that the command wait for the replication to complete. This can potentially take a very long time.
				- Example
					hdfs dfs -setrep -w 3 /user/hadoop/dir1
			- hdfs dfs -touchz URI [URI ...]
				- creates zero size file
			-  hdfs dfs -text <src>
				- Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.
			 - hdfs dfs -test -[ezd] URI
				Options:
					The -e option will check to see if the file exists, returning 0 if true.
					The -z option will check to see if the file is zero length, returning 0 if true.
					The -d option will check to see if the path is directory, returning 0 if true.
			-  hdfs dfs -tail [-f] URI
				Displays last kilobyte of the file to stdout.
			- hdfs dfs -stat URI [URI ...]
				Returns the stat information on the path.

- Short-Circuit read
	- Client program and datanode communicate over tcp sockets. It is much more faster to communicate via shared memory if both client and datanode are on the same machine. With below settings it is possible that they use linux sockets and use shared memory. It requires native lib.

			<configuration>
		  <property>
		    <name>dfs.client.read.shortcircuit</name>
		    <value>true</value>
		  </property>
		  <property>
		    <name>dfs.domain.socket.path</name>
		    <value>/var/lib/hadoop-hdfs/dn_socket</value>
		  </property>
		</configuration>

- Native library support
	- Compression Codecs (bzip2, lz4, snappy, zlib)
	- Native IO utilities for "HDFS Short-Circuit Local Reads" and "Centralized Cache Management in HDFS"
	- CRC32 checksum implementation
- Edits file viewer
	- Command
		 bin/hdfs oev -i edits -o edits.xml
	- Edits file viewer can be used to open edits file in xml format and convert the xml format back to binary one
	- Typical usage is to correct the currupt edits file.
	- Sometimes edits file just misses the end tag, which can be added with appending the below code and then converting it back to binary file
		 <RECORD>
    		<OPCODE>-1</OPCODE>
    		<DATA></DATA>
  		</RECORD>
- Centralized Cache Management in HDFS

- Java interface
	- Important classes
		- FileSystem 
		- FileContext
		- FSDataInputStream
		- FSDataOutputStream
- Write
	-It’s possible, but unlikely, that multiple datanodes fail while a block is being written. As long as dfs.namenode.replication.min replicas (which defaults to one) are written, the write will succeed, and the block will be asynchronously replicated across the cluster until its target replication factor is reached (dfs.replication, which defaults to three).
	- When the client has finished writing data, it calls close() on the stream (step 6). This action flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete (step 7). The namenode already knows which blocks the file is made up of (via DataStreamer asking for block allocations), so it only has to wait for blocks to be minimally replicated before returning successfully.
	- Hadoop’s default strategy is to place the first replica on the same node as the client (for clients running outside the cluster, a node is chosen at random, although the system tries not to pick nodes that are too full or too busy). The second replica is placed on a different rack from the first (off-rack), chosen at random. The third replica is placed on the same rack as the second, but on a different node chosen at random. Further replicas are placed on random nodes on the cluster, although the system tries to avoid placing too many replicas on the same rack.
	- Overall, this strategy gives a good balance among reliability (blocks are stored on two racks), write bandwidth (writes only have to traverse a single network switch), read performance (there’s a choice of two racks to read from), and block distribution across the cluster (clients only write a single block on the local rack).
- Coherency
	- After creating a file, it is visible in the filesystem namespace
		Path p = new Path("p");
    	fs.create(p);
    	assertThat(fs.exists(p), is(true));
    -  any content written to the file is not guaranteed to be visible, even if the stream is flushed. So the file appears to have a length of zero:
    	Path p = new Path("p");
    	OutputStream out = fs.create(p);
    	out.write("content".getBytes("UTF-8"));
    	out.flush();
    	assertThat(fs.getFileStatus(p).getLen(), is(0L));
    - Once more than a block’s worth of data has been written, the first block will be visible to new readers. This is true of subsequent blocks, too: it is always the current block being written that is not visible to other readers.
    - HDFS provides a method for forcing all buffers to be flushed to the datanodes via the hflush() method on FSDataOutputStream. After a successful return from hflush(), HDFS guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers:
    	 Path p = new Path("p");
		FSDataOutputStream out = fs.create(p);
    	out.write("content".getBytes("UTF-8"));
    	out.hflush();
    	assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
    - Note that hflush() does not guarantee that the datanodes have written the data to disk, only that it’s in the datanodes’ memory (so in the event of a data center power outage, for example, data could be lost). For this stronger guarantee, use hsync() instead.The behavior of hsync() is similar to the fsync system call in POSIX that commits buffered data for a file descriptor. For example, using the standard Java API to write a local file, we are guaranteed to see the content after flushing the stream and synchronizing:
    	 FileOutputStream out = new FileOutputStream(localFile);
	    out.write("content".getBytes("UTF-8"));
	    out.flush(); // flush to operating system
	    out.getFD().sync(); // sync to disk
	    assertThat(localFile.length(), is(((long) "content".length())));
	- Closing the file performs hflush() too
		Path p = new Path("p");
	    OutputStream out = fs.create(p);
	    out.write("content".getBytes("UTF-8"));
	    out.close();
	    assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));
- Parallel copy can be achived command (Uses Map-Reduce to copy)
	hadoop distcp file1 file2
- When copying data into HDFS, it’s important to consider cluster balance. HDFS works best when the file blocks are evenly spread across the cluster, so you want to ensure that distcp doesn’t disrupt this. For example, by specifying -m 1, a single map would do the copy, which—apart from being slow and not using the cluster resources efficiently—would mean that the first replica of each block would reside on the node running the map (until the disk filled up). The second and third replicas would be spread across the cluster, but this one node would be unbalanced. By having more maps than nodes in the cluster, this problem is avoided. For this reason, it’s best to start by running distcp with the default of 20 maps per node.
- Use below command to findout the blocks of the files, this also includes the datanode address
	hdfs fsck /user/tom/part-00007 -files -blocks -racks 
- Datanode block scanner
	- Every datanode runs a block scanner, which periodically verifies all the blocks stored on the datanode. This allows bad blocks to be detected and fixed before they are read by clients. The scanner maintains a list of blocks to verify and scans them one by one for checksum errors. It employs a throttling mechanism to preserve disk bandwidth on the datanode.
	- Blocks are periodically verified every three weeks to guard against disk errors over time (this is controlled by the dfs.datanode.scan.period.hours property, which defaults to 504 hours). Corrupt blocks are reported to the namenode to be fixed.
	- You can get a block verification report for a datanode by visiting the datanode’s web interface at http://datanode:50075/blockScannerReport. 
	- By specifying the listblocks parameter, http://datanode:50075/blockScannerReport?listblocks, the report is preceded by a list of all the blocks on the datanode along with their latest verification status. Here
	-  The type of scan is local if it was performed by the background thread, remote if it was performed by a client or a remote datanode, or none if a scan of this block has yet to be made. 
- Balancer
	- Over time, the distribution of blocks across datanodes can become unbalanced. An unbalanced cluster can affect locality for MapReduce, and it puts a greater strain on the highly utilized datanodes, so it’s best avoided.
-----------------------------------------------------------------------------------
Checksum verification
	- HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A separate checksum is created for every dfs.bytes-per-checksum bytes of data. The default is 512 bytes, and because a CRC-32C checksum is 4 bytes long, the storage overhead is less than 1%.
	- A client writing data sends it to a pipeline of datanodes and the last datanode in the pipeline verifies the checksum. If the datanode detects an error, the client receives an subclass of IOException.
	- When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored at the datanode. Each datanode keeps a persistent log of checksum verifications, so it knows the last time each of its blocks was verified. When a client successfully verifies a block, it tells the datanode
	- Aside from block verification on client reads, each datanode runs a DataBlockScanner in a background thread that periodically verifies all the blocks stored on the datanode. This is to guard against corruption due to “bit rot” in the physical storage media.
	- Because HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of the good replicas to produce a new, uncorrupt replica. The way this works is that if a client detects an error when reading a block, it reports the bad block and the datanode it was trying to read from to the namenode before throwing a ChecksumException. The namenode marks the block replica as corrupt so it doesn’t direct clients to it or try to copy this replica to another datanode. It then schedules a copy of the block to be replicated on another datanode, so its replication factor is back at the expected level. Once this has happened, the corrupt replica is deleted.
	- It is possible to disable verification of checksums by passing false to the setVerifyChecksum() method on FileSystem before using the open() method to read a file. The same effect is possible from the shell by using the -ignoreCrc option with the -get or the equivalent -copyToLocal command. This feature is useful if you have a corrupt file that you want to inspect so you can decide what to do with it. For example, you might want to see whether it can be salvaged before you delete it.
	- You can find a file’s checksum with hadoop fs -checksum. This is useful to check whether two files in HDFS have the same contents, something that distcp does, for example (see Parallel Copying with distcp).
LocalFileSystem
	- The Hadoop LocalFileSystem performs client-side checksumming. This means that when you write a file called filename, the filesystem client transparently creates a hidden file, .filename.crc, in the same directory containing the checksums for each chunk of the file. The chunk size is controlled by the file.bytes-per-checksum property, which defaults to 512 bytes. The chunk size is stored as metadata in the .crc file, so the file can be read back correctly even if the setting for the chunk size has changed. Checksums are verified when the file is read, and if an error is detected, LocalFileSystem throws a ChecksumException.
-----------------------------------------------------------------------------------
Compression
	- Type:DEFLATE 	- File-extension:.deflat 	isSplittable:no 	tool:N/A 	algorithm:DEFLATE
	- Type:gzip 	- File-extension:.gz 		isSplittable:no 	tool:gzip 	algorithm:DEFLATE
	- Type:bzip2 	- File-extension:.bzip2		isSplittable:yes 	tool:bzip2 	algorithm:bzip2
	- Type:LZO 		- File-extension:.lzop 		isSplittable:no 	tool:lzop 	algorithm:LZO
	- Type:LZ4 		- File-extension:.lz4 		isSplittable:no 	tool:N/A 	algorithm:LZ4
	- Type:Snappy	- File-extension:.snappy	isSplittable:no 	tool:N/A 	algorithm:Snappy
	- The different tools have very different compression characteristics. Gzip is a general-purpose compressor and sits in the middle of the space/time trade-off. Bzip2 compresses more effectively than gzip, but is slower. Bzip2’s decompression speed is faster than its compression speed, but it is still slower than the other formats. LZO, LZ4. and Snappy, on the other hand, all optimize for speed and are around an order of magnitude faster than gzip, but compress less effectively. Snappy and LZ4 are also significantly faster than LZO for decompression
	- Codecs
		A codec is the implementation of a compression-decompression algorithm. In Hadoop, a codec is represented by an implementation of the CompressionCodec interface. So, for example, GzipCodec encapsulates the compression and decompression algorithm for gzip. Table 5-2 lists the codecs that are available for Hadoop.
	- Codecs
		DEFLATE	org.apache.hadoop.io.compress.DefaultCodec
		gzip	org.apache.hadoop.io.compress.GzipCodec
		bzip2	org.apache.hadoop.io.compress.BZip2Codec
		LZO		com.hadoop.compression.lzo.LzopCodec
		LZ4		org.apache.hadoop.io.compress.Lz4Codec
		Snappy	org.apache.hadoop.io.compress.SnappyCodec
	- Compressing and decompressing streams with CompressionCodec
		- CompressionCodec has two methods that allow you to easily compress or decompress data. To compress data being written to an output stream, use the createOutputStream(OutputStream out) method to create a CompressionOutputStream to which you write your uncompressed data to have it written in compressed form to the underlying stream. Conversely, to decompress data being read from an input stream, call createInputStream(InputStream in) to obtain a CompressionInputStream, which allows you to read uncompressed data from the underlying stream.
		- CompressionOutputStream and CompressionInputStream are similar to java.util.zip.DeflaterOutputStream and java.util.zip.DeflaterInputStream, except that both of the former provide the ability to reset their underlying compressor or decompressor, which is important for applications that compress sections of the data stream as separate blocks, such as SequenceFile, described in SequenceFile.
		- CompressionOutputStream and CompressionInputStream are similar to java.util.zip.DeflaterOutputStream and java.util.zip.DeflaterInputStream, except that both of the former provide the ability to reset their underlying compressor or decompressor, which is important for applications that compress sections of the data stream as separate blocks, such as SequenceFile, described in SequenceFile.
	- If we need to use multiple compressorInputStrems then we shall use it with separate Compressor. After every compressor work is done we can use reset() to reuse it.CodecPool can be used to this work
	- To compress the output of a MapReduce job, in the job configuration, set the mapreduce.output.fileoutputformat.compress property to true and the mapreduce.output.fileoutputformat.compress.codec property to the classname of the compression codec you want to use. Alternatively, you can use the static convenience methods on FileOutputFormat
		FileOutputFormat.setCompressOutput(job, true);
    	FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
    - If you are emitting sequence files for your output, you can set the mapreduce.output.fileoutputformat.compress.type property to control the type of compression to use. The default is RECORD, which compresses individual records. 
    - To compress map output set mapreduce.map.output.compress property	to true (default is false ) and set mapreduce.map.output.compress.codec	to the compression codec. code:
    	conf.setBoolean(Job.MAP_OUTPUT_COMPRESS, true);
    	conf.setClass(Job.MAP_OUTPUT_COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);

